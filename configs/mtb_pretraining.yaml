# Experiment
experiment_name: pretraining
# Data
data: data/cnn.txt #pre-training data.txt file path
entities_of_interest:
  - PERSON
  - NORP
  - FAC
  - ORG
  - GPE
  - LOC
  - PRODUCT
  - EVENT
  - WORK_OF_ART
  - LAW
  - LANGUAGE
normalization:
  - lowercase
  - html
  - urls
min_pool_size: 8
# Model
transformer: bert-base-uncased
# Training
batch_size: 32 # Training batch size
gradient_acc_steps: 2 # steps of gradient accumulation
max_norm: 1.0 # Clipped gradient norm
epochs: 25 # Number of Epochs
lr: 0.0001 # learning rate
resume: False
